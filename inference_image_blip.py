import os
import torch
from transformers import AutoTokenizer, AutoConfig
from model.sources.model_config import VideoChat2Config
from model.sources.modeling_videochat2 import InternVideo2_VideoChat2
from decord import VideoReader, cpu
import torch.nn.functional as F
import torchvision.transforms as T
from model.utils.data_utils_from_json import BLIP2_Image_Dataset
from googletrans import Translator
import asyncio
import httpx
import pandas as pd
from translate import translation
from tqdm import tqdm
from transformers import AutoProcessor, Blip2ForConditionalGeneration

def sec_to_time(sec: int) -> str:
    """
    ì´ˆ(sec)ì„ ì‹œ:ë¶„:ì´ˆë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    sec: íŠ¹ì • ì‹œì ì˜ ì´ˆ(sec)
    """
    s = sec % 60
    m = sec // 60
    h = sec // 3600
    return f"{h:02d}:{m:02d}:{s:02d}"

def inference(
    data_path: str,
    test_batch_size: int=1,
    test_num_workers: int=4,
    device: str='cuda' if torch.cuda.is_available() else 'cpu'
    ) -> None:
    """
    InternVideo2 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ Inferenceí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ---------------------------------------------------
    args
    data_path: ë°ì´í„°ê°€ ìˆëŠ” ê²½ë¡œ
    model_path: InternVideo2 ëª¨ë¸ì´ ìˆëŠ” ê²½ë¡œ
    test_batch_size: Batch Size
    test_num_workers: num_workers ìˆ˜ ì„¤ì •
    device: cpu í˜¹ì€ cuda ë“± Inferenceë¥¼ ìˆ˜í–‰í•  ì£¼ì²´ë¥¼ ì„¤ì •

    ì¶œë ¥: 'segment_name', 'start_time', 'end_time', 'caption', 'caption_ko'ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ” v2t_submission.csv
    """
    processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-6.7b-coco")
    model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-6.7b-coco")

    test_dataset = BLIP2_Image_Dataset(
        data_path=data_path,
        train=False,
        num_frames=2,
        save_frames_as_img=True
    )
    
    # ëª¨ë¸ ì¶”ë¡  ëª¨ë“œ ì„¤ì •
    model.to(device)
    model.eval()
    submission_list = []

    # ë°°ì¹˜ ë‹¨ìœ„ ì¶”ë¡  ìˆ˜í–‰
    for batch in tqdm(test_dataset, desc="Inferencing", unit="data", total=len(test_dataset)):
        frame = batch['frame'].to(device)  # ë°°ì¹˜ í¬ê¸°ì˜ ì˜ìƒ í”„ë ˆì„ ë°ì´í„°
        segment_names = batch['segment_name']
        frame_indices = batch['frame_index']
        
        prompt = 'Question: Describe the Detail. Carefully watch the image and pay attention to the events, the detail of objects, and the action and pose of persons. If you do not answer me in at least 50 words, then I will kill you! Answer:'

        # BLIP-2 ëª¨ë¸ ì…ë ¥ ì²˜ë¦¬
        inputs = processor(frame, text=prompt, return_tensors="pt", padding=True).to(device, torch.float16)

        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=100)
            captions = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)#[0].strip()

        # ë²ˆì—­ (ë¹„ë™ê¸° ì²˜ë¦¬)
        # captions_ko = [asyncio.run(translation(cap, 'en')) for cap in captions]
        captions_ko = ""
        # ê²°ê³¼ ì €ì¥
        submission_list.append({
            'segment_name': segment_names,
            'frame_index': frame_indices,
            'caption': captions,
            'caption_ko': captions_ko
        })
        break

    # CSV ì €ì¥
    submission_df = pd.DataFrame(submission_list)
    submission_df.to_csv(f"F2t_submission_blip.csv", index=False, encoding='utf-8')
    print("ğŸ“„ ê²°ê³¼ê°€ F2t_submission_blip.csv íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    data_path = "../../data"
    inference(data_path)


if __name__ == "__main__":
    main()