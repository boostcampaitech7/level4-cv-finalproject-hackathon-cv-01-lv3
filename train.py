import os
import torch
from transformers import AutoTokenizer, AutoConfig
from model.sources.model_config import VideoChat2Config
from model.sources.modeling_videochat2 import InternVideo2_VideoChat2
from decord import VideoReader, cpu
import torch.nn.functional as F
import torchvision.transforms as T
from model.utils.data_utils_from_json import InternVideo2_VideoChat2_Dataset, InternVideo2_VideoChat2_DataLoader
from tqdm import tqdm
# BERTScore ê³„ì‚°ì„ ìœ„í•¨ (ì‚¬ìš© ì‹œ, pip install bert_score ì´í›„, ì•„ë˜ 1ì¤„ ì£¼ì„ í•´ì œ)
from bert_score import score
import wandb
import json
from datetime import datetime

def train(
    model_path: str,
    data_path: str = "../../data",
    num_epochs: int = 50,
    train_batch_size: int = 2,
    test_batch_size: int = 1,
    train_num_workers: int = 4,
    test_num_workers: int = 4,
    learning_rate: float = 1e-4,
    weight_decay: float = 1e-2,
    validation_interval: int = 5,
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
):
    """
    ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜, ì¼ì • ì£¼ê¸°ë¡œ validationì„ ìˆ˜í–‰í•¨.

    Args: 
        model_path: ëª¨ë¸ ê²½ë¡œ
        data_path: ë°ì´í„° ê²½ë¡œ
        num_epochs: í•™ìŠµ ì£¼ê¸°
        train_batch_size: í•™ìŠµ ë°°ì¹˜ í¬ê¸°
        test_batch_size: ê²€ì¦ ë°°ì¹˜ í¬ê¸°
        train_num_workers: í•™ìŠµ ë°ì´í„° ë¡œë” ìŠ¤ë ˆë“œ ìˆ˜
        test_num_workers: ê²€ì¦ ë°ì´í„° ë¡œë” ìŠ¤ë ˆë“œ ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        weight_decay: ê°€ì¤‘ì¹˜ ê°ì‡  ë¹„ìœ¨
        validation_interval: ê²€ì¦ ì£¼ê¸°
        device: CPU or GPU
    """

    # ë¡œê¹…ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    current_dir = os.path.dirname(os.path.abspath(__file__))
    log_dir = os.path.join(current_dir, 'logs', datetime.now().strftime('%Y%m%d_%H%M%S'))
    os.makedirs(log_dir, exist_ok=True)

    # ê¸°ë³¸ ì„¤ì • ë¡œê¹…
    config = {
        "model_path": model_path,
        "data_path": data_path,
        "num_epochs": num_epochs,
        "train_batch_size": train_batch_size,
        "test_batch_size": test_batch_size,
        "learning_rate": learning_rate,
        "weight_decay": weight_decay,
        "device": device,
        "system": {
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_device": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None"
        }
    }
    
    with open(os.path.join(log_dir, 'config.json'), 'w') as f:
        json.dump(config, f, indent=4)

    # wandb ì´ˆê¸°í™”
    wandb.init(
        project="videochat2-training",
        config=config
    )

    current_dir = os.path.dirname(os.path.abspath(__file__))
    config = VideoChat2Config.from_json_file(
        os.path.join(current_dir,'model','configs', 'config.json')
    )

    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (Mistral-7B)
    tokenizer = AutoTokenizer.from_pretrained(
        config.model_config.llm.pretrained_llm_path,
        trust_remote_code=True,
        use_fast=False,
        token=os.getenv('HF_TOKEN')
    )
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = InternVideo2_VideoChat2.from_pretrained(
        model_path,
        config=config,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    ).to(device)

    # ëª¨ë¸ ë‚´ë¶€ì ìœ¼ë¡œ Attention, Loss, Output ì¶œë ¥ ë“±ì—ì„œ pad_tokenì´ ë¶ˆí•„ìš”í•œ ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ì„¤ì •
    model.config.pad_token_id = tokenizer.pad_token_id

    train_dataset = InternVideo2_VideoChat2_Dataset(
        data_path=data_path,
        use_segment=True,
        use_audio=False,
        train=True
    )

    test_dataset = InternVideo2_VideoChat2_Dataset(
        data_path=data_path,
        use_segment=True,
        use_audio=False,
        train=False
    )
    
    train_loader = InternVideo2_VideoChat2_DataLoader(
        dataset=train_dataset,
        batch_size=train_batch_size,
        num_workers=train_num_workers,
        shuffle=True,
        pin_memory=True,
        use_audio=False
    )
    
    test_loader = InternVideo2_VideoChat2_DataLoader(
        dataset=test_dataset,
        batch_size=test_batch_size,
        num_workers=test_num_workers,
        shuffle=False,
        pin_memory=True,
        use_audio=False
    )
    
    optimizer, scheduler = model.prepare_for_training(
        learning_rate=learning_rate,
        weight_decay=weight_decay
    )

    query_embedding_size = model.query_tokens.shape[1] + model.extra_query_tokens.shape[1]

    # ëª¨ë¸ ì •ë³´ ë¡œê¹…
    model_info = {
        "total_params": sum(p.numel() for p in model.parameters()),
        "trainable_params": sum(p.numel() for p in model.parameters() if p.requires_grad),
        "model_structure": str(model)
    }
    
    with open(os.path.join(log_dir, 'model_info.json'), 'w') as f:
        json.dump(model_info, f, indent=4)

    # í•™ìŠµ ë¡œê·¸ íŒŒì¼ ìƒì„±
    log_file = os.path.join(log_dir, 'training.log')
    with open(log_file, 'w') as f:
        f.write(f"Training started at {datetime.now()}\n")

    # ë² ìŠ¤íŠ¸ ìŠ¤ì½”ì–´ ì¶”ì  ë³€ìˆ˜ ì¶”ê°€
    best_val_score = -float('inf')
    
    for epoch in range(num_epochs):
        train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')
        epoch_loss = 0.0
        for batch_idx, batch in enumerate(train_loop):
            frames = batch['frames'].to(device)
            annotations = batch['annotations']
            
            # í…ìŠ¤íŠ¸ í† í°í™”
            text_inputs = tokenizer(
                annotations,
                padding='max_length',
                truncation=True,
                max_length=256,
                return_tensors="pt"
            ).to(device)

            optimizer.zero_grad()
            outputs, _ = model(
                input_ids=text_inputs.input_ids,
                attention_mask=text_inputs.attention_mask,
                video=frames,
                labels=text_inputs.input_ids,
                video_idx=torch.ones(frames.shape[0], query_embedding_size).to(device)
            )

            loss = outputs.loss
            epoch_loss += loss.item()
            loss.backward()
            optimizer.step()
            scheduler.step()

            train_loop.set_postfix(loss=f'{loss.item():.4f}, batch_idx: {batch_idx}')
            
            # ë¡œê¹…ì„ ë¨¼ì € ìˆ˜í–‰ í›„ ë³€ìˆ˜ ì‚­ì œ
            with open(log_file, 'a') as f:
                f.write(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\n")

            # ë©”ëª¨ë¦¬ í•´ì œ
            del frames, annotations, text_inputs, outputs, loss

        # ì—í­ë³„ í‰ê·  ì†ì‹¤ ë¡œê¹…
        avg_epoch_loss = epoch_loss / len(train_loader)
        wandb.log({
            "epoch": epoch,
            "epoch_loss": avg_epoch_loss
        })

        if epoch % validation_interval == 0:
            print("--------------------------------")
            print(f"validation start, epoch: {epoch+1}")
            val_score = validation(model, test_loader, tokenizer, device, query_embedding_size)
            
            # ë² ìŠ¤íŠ¸ ìŠ¤ì½”ì–´ ê°±ì‹  ì‹œ ëª¨ë¸ ì €ì¥
            if val_score > best_val_score:
                best_val_score = val_score
                save_model(
                    model, 
                    optimizer=optimizer, 
                    epoch=epoch,
                    val_score=val_score,  # ìƒˆ íŒŒë¼ë¯¸í„° ì¶”ê°€
                    save_path=os.path.join('temp_model', 'best_model.pt')
                )
                print(f"ğŸ”¥ New best model saved with score: {val_score:.4f}")
            
            # validation ê²°ê³¼ ë¡œê¹… 
            wandb.log({
                "epoch": epoch,
                "validation_score": val_score
            })
            
            print(f"validation end, epoch: {epoch+1}")
            print("--------------------------------")
            model.train()

            with open(log_file, 'a') as f:
                f.write(f"Validation - Epoch {epoch}: Score = {val_score:.4f}\n")
                f.write("-" * 50 + "\n")

    wandb.finish()

def save_model(
    model, 
    optimizer=None, 
    scheduler=None, 
    epoch=None, 
    val_score=None,  # ìƒˆ íŒŒë¼ë¯¸í„° ì¶”ê°€
    loss=None, 
    save_path="best_model.pt"
):
    """
    ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜, optimizer, scheduler, epoch, lossë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤

    Args:
        model (torch.nn.Module): The model to save.
        optimizer (torch.optim.Optimizer, optional): Optimizer state to save. Default is None.
        scheduler (torch.optim.lr_scheduler, optional): Scheduler state to save. Default is None.
        epoch (int, optional): Current epoch. Default is None.
        loss (float, optional): Best validation loss. Default is None.
        save_path (str, optional): File path to save the model. Default is "best_model.pt".
    """
    # Ensure the save directory exists
    save_dir = os.path.dirname(save_path)
    if not os.path.exists(save_dir) and save_dir != "":
        os.makedirs(save_dir)
    
    # Prepare the state dictionary
    state = {
        'model_state_dict': model.state_dict(),
        'best_val_score': val_score,  # ê²€ì¦ ì ìˆ˜ ì €ì¥
    }
    if optimizer:
        state['optimizer_state_dict'] = optimizer.state_dict()
    if scheduler:
        state['scheduler_state_dict'] = scheduler.state_dict()
    if epoch is not None:
        state['epoch'] = epoch
    if loss is not None:
        state['best_loss'] = loss
    
    # Save the state dictionary
    torch.save(state, save_path)
    print(f"Model saved to {save_path} | Best Score: {val_score:.4f}")

def validation(model, dataloader, tokenizer, device, query_embedding_size):
    model.eval()
    total_score = 0
    val_loop = tqdm(dataloader, desc='Validation')
    with torch.no_grad():
        for batch in val_loop:
            frames = batch['frames'].to(device)
            annotations = batch['annotations']
            
            # ì´í›„ í‰ê°€ ë©”íŠ¸ë¦­ ë„ì…í•˜ì—¬ ëª¨ë¸ ì €ì¥ í•„ìš”
            # text_inputs = tokenizer(
            #     annotations,
            #     padding='longest',
            #     truncation=True,
            #     max_length=256,
            #     return_tensors="pt"
            # ).to(device)

            # _, text_embeds = model(
            #     input_ids=text_inputs.input_ids,
            #     attention_mask=text_inputs.attention_mask,
            #     video=frames,
            #     labels=text_inputs.input_ids,
            #     video_idx=torch.ones(frames.shape[0], query_embedding_size).to(device)
            # )
            
            generation_config = {
                'num_beams': 1,            # ë¹” ì„œì¹˜ í¬ê¸°
                'max_new_tokens': 200,     # ìµœëŒ€ ìƒì„± ê¸¸ì´
                'do_sample': False,         # ìƒ˜í”Œë§ ì‚¬ìš©
                'top_p': 0.9,               # ìƒ˜í”Œë§ í™•ë¥ (probabilistic)
                'top_k': None,              # ìƒ˜í”Œë§ í™•ë¥ (greedy)
                'temperature': 1.0,          # ìƒ˜í”Œë§ ì˜¨ë„
                'length_penalty': 1,         # ê¸¸ì´ íŒ¨ë„í‹°
                'repetition_penalty': 1.0    # ë°˜ë³µ íŒ¨ë„í‹°
            }

            # ìº¡ì…˜ ìƒì„±
            response, _ = model.chat(
                tokenizer=tokenizer,
                msg='',
                user_prompt='Describe the video step by step',
                instruction="Carefully watch the video and describe what is happening in detail.",
                media_type='video',
                media_tensor=frames,
                chat_history=[],
                return_history=True,
                generation_config=generation_config
            )
            # BERTScoreì„ í™œìš©í•˜ì—¬ GTì™€ Prediction ë¹„êµ. ì‚¬ìš©ì‹œ ì•„ë˜ 2ì¤„ ì£¼ì„ í•´ì œ (í•„ìš” ì‹œ, Baseline í‰ê°€ Metricìœ¼ë¡œ í™œìš©)
            P, R, F1 = score([response], [batch['annotations']], lang="en")
            total_score += F1[0]
            print(f"response: {response}")

    avg_score = total_score / len(dataloader)
    # ë§Œì•½, BERTScore ë“±ì˜ í‰ê°€ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ë¡œ total_scoreë¥¼ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤ë©´ avg_scoreëŠ” ê³„ì† 0ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì´ ì •ìƒì„
    print(F"avg_score: {avg_score}")
    return avg_score

def main():
    # wandb ë¡œê·¸ì¸
    wandb.login()
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = os.path.join(current_dir, "model/weights")
    
    # ë¹„ë””ì˜¤ ê²½ë¡œ ì„¤ì •
    video_path = os.path.join(current_dir, "../../data")

    train(model_path, video_path)

if __name__ == "__main__":
    main()
